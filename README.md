Отличная идея. Качественный README — это лицо проекта. Вот подробное описание архитектуры и ее возможностей, отформатированное в стиле Markdown для вашего файла `README.md`.

---

# Иерархическое Представление Временных Рядов (Hierarchical Time Series Representation Learning)

Этот проект представляет собой реализацию продвинутой нейросетевой архитектуры для решения двух задач одновременно (Multi-Task Learning) на данных временных рядов, имеющих иерархическую структуру:

1.  **Обучение Представлений (Representation Learning):** Создание информативных векторных представлений (эмбеддингов) для каждого временного ряда. Эти эмбеддинги кодируют принадлежность сигнала к определенному "измерительному элементу" и конкретному "замеру" (сессии).
2.  **Детекция Ключевых Точек (Keypoint Detection):** Определение точных временных позиций заранее заданных "ключевых событий" внутри каждого сигнала.

Модель идеально подходит для анализа сенсорных данных, биометрических сигналов, логов оборудования и других временных рядов, где данные естественным образом сгруппированы по нескольким уровням (например, *Оборудование -> Рабочая смена -> Цикл работы*).

## Проблема и Подход

Представим данные со следующей структурой:
*   **Измерительный Элемент:** Уникальный объект, генерирующий сигналы (например, конкретный станок, человек, датчик).
*   **Замер (Сессия):** Группа сигналов, снятых с одного элемента в рамках одной сессии или в схожих условиях (например, все циклы работы станка за один час).
*   **Сигнал:** Отдельный временной ряд (например, один цикл работы станка).

**Цель:** Научить модель понимать эту иерархию. Эмбеддинги сигналов из одного замера должны быть очень близки друг к другу. Эмбеддинги из разных замеров, но одного элемента, должны быть тоже близки, но дальше, чем сигналы из одного замера. Наконец, эмбеддинги от разных элементов должны быть максимально удалены друг от друга в векторном пространстве.



Для этого мы используем **Иерархическое Метрическое Обучение (Hierarchical Metric Learning)** с помощью модифицированной триплетной функции потерь.

## Архитектура Модели (`HierarchicalSignalNet`)

Модель состоит из общего "ствола" (backbone) для извлечения признаков и двух отдельных "голов" для решения двух задач.



### 1. Сверточный Ствол (Convolutional Backbone)

*   **Назначение:** Извлечение локальных, инвариантных к сдвигу признаков из сырого временного ряда. Свертки действуют как настраиваемые фильтры, которые учатся распознавать характерные паттерны (пики, спады, осцилляции).
*   **Реализация:** Последовательность блоков `Conv1D`, `BatchNorm1d`, `ReLU` и `MaxPool1d`.
    *   `Conv1D`: Применяет фильтры вдоль временной оси.
    *   `BatchNorm1d`: Стабилизирует и ускоряет обучение.
    *   `ReLU`: Нелинейная функция активации.
    *   `MaxPool1d`: Уменьшает временну́ю размерность, делая представление более устойчивым к небольшим сдвигам во времени.

### 2. Рекуррентная Голова (Recurrent Encoder)

*   **Назначение:** Анализ последовательности локальных признаков, извлеченных сверточным стволом. Рекуррентная сеть (GRU) способна улавливать долгосрочные зависимости и контекст во всем сигнале.
*   **Реализация:** Двунаправленная `GRU` (Gated Recurrent Unit).
    *   **Двунаправленность** позволяет сети анализировать последовательность как в прямом, так и в обратном направлении, что дает более полное представление о контексте в каждой временной точке.

### 3. Выходные Головы (Output Heads)

После GRU поток данных разделяется на две ветви:

#### а) Голова для Эмбеддингов (Embedding Head)

*   **Назначение:** Сформировать итоговый векторный эмбеддинг для всего сигнала.
*   **Реализация:**
    1.  Берется **последнее скрытое состояние** `GRU` (которое суммирует информацию о всей последовательности).
    2.  Оно пропускается через `Linear` слой для проецирования в целевое пространство эмбеддингов (например, размерности 64 или 128).
    3.  Применяется **L2-нормализация** (`F.normalize`), которая размещает все эмбеддинги на поверхности гиперсферы единичного радиуса. Это стандартная практика в Metric Learning, улучшающая стабильность обучения.

#### б) Голова для Детекции Ключевых Точек (Keypoint Detection Head)

*   **Назначение:** Для каждого типа ключевой точки предсказать ее наиболее вероятную позицию во времени.
*   **Реализация:**
    1.  Берется **полная последовательность выходов** `GRU` (размерности `(batch, seq_len, rnn_hidden_size)`).
    2.  Она пропускается через `Linear` слой, который для каждой временной точки выдает логит (оценку вероятности) того, что здесь находится ключевая точка.
    3.  Выход имеет размерность `(batch, seq_len, num_keypoints)`, представляя собой "карту вероятностей" для каждой ключевой точки вдоль всего сигнала.



### Стратегия Обучения

Эффективность модели в значительной степени определяется комбинированной функцией потерь (`HierarchicalLoss`) и умной стратегией формирования батчей (`HierarchicalSampler`).

#### 1. Комбинированная Функция Потерь (`HierarchicalLoss`)

Мы используем общую функцию потерь, которая является взвешенной суммой потерь для каждой из двух задач:

\[ L_{\text{total}} = L_{\text{keypoint}} + \lambda \cdot L_{\text{metric}} \]

*   **`L_keypoint` (Cross-Entropy Loss):** Это стандартная функция потерь для задач классификации. Для каждой ключевой точки она рассматривает временной ряд как последовательность, где нужно классифицировать каждый временной шаг — является ли он искомой точкой или нет. Это заставляет "голову детекции ключевых точек" генерировать высокие значения вероятности на правильных временных позициях.

*   **`L_metric` (Hierarchical Triplet Loss):** Это ядро нашего подхода к обучению представлений. Мы одновременно используем два типа "триплетов", чтобы научить модель иерархии:

    *   **Триплет Уровня Замера:**
        *   **Якорь (Anchor):** Случайный сигнал из батча.
        *   **Позитив (Positive):** Другой сигнал, взятый из **того же замера**, что и якорь.
        *   **Негатив Замера (Session Negative):** Сигнал, взятый из **другого замера**, но **того же элемента**, что и якорь.

        Эта потеря (`loss_session`) заставляет модель сближать эмбеддинги сигналов из одного замера и отдалять их от эмбеддингов других замеров того же элемента.
        \[ L_{\text{session}} = \max(0, D(a, p) - D(a, n_{\text{session}}) + \text{margin}_{\text{session}}) \]

    *   **Триплет Уровня Элемента:**
        *   **Якорь (Anchor):** Тот же самый случайный сигнал.
        *   **Позитив (Positive):** Тот же самый позитивный пример из того же замера.
        *   **Негатив Элемента (Element Negative):** Сигнал, взятый от **совершенно другого измерительного элемента**.

        Эта потеря (`loss_element`) заставляет модель сближать эмбеддинги сигналов от одного элемента и максимально отдалять их от эмбеддингов других элементов.
        \[ L_{\text{element}} = \max(0, D(a, p) - D(a, n_{\text{element}}) + \text{margin}_{\text{element}}) \]

    Итоговая метрическая потеря `L_metric` — это сумма `loss_session` и `loss_element`. Гиперпараметр `λ` контролирует важность метрического обучения по сравнению с детекцией ключевых точек.

#### 2. Стратегия Формирования Батчей (`HierarchicalSampler`)

Случайная выборка данных для такой сложной функции потерь неэффективна. Вероятность того, что в случайно собранном батче окажутся все необходимые типы примеров (из одного замера, из разных замеров, от разных элементов), очень мала.

Поэтому мы используем кастомный **`HierarchicalSampler`**. Он работает по принципу "P-K":
*   Для каждого батча он сначала случайно выбирает **`P`** уникальных измерительных элементов.
*   Затем для каждого из этих `P` элементов он случайно выбирает **`K`** сигналов.

Это **гарантирует**, что каждый батч имеет богатую иерархическую структуру, содержащую множество потенциальных триплетов обоих уровней. Такой подход делает обучение стабильным и эффективным.

## Возможности и Применение

*   **Поиск Похожих Сигналов:** Получив эмбеддинг для нового сигнала, можно быстро найти наиболее похожие на него сигналы в базе данных (например, для диагностики аномалий, находя сигналы, не похожие ни на один из "здоровых" эталонов).
*   **Классификация Элементов:** Эмбеддинги можно напрямую использовать как признаки для обучения простого классификатора (например, k-NN или SVM) для определения, какому измерительному элементу принадлежит новый сигнал.
*   **Синхронизация и Выравнивание:** Предсказанные ключевые точки можно использовать для выравнивания сигналов во времени, что полезно для их дальнейшего совместного анализа или усреднения.
*   **Контроль Качества:** Модель может отслеживать "дрейф" характеристик оборудования, анализируя, как со временем смещаются кластеры эмбеддингов для одного и того же элемента.

## Как Использовать

1.  **Установка зависимостей:**
    ```bash
    pip install torch numpy
    ```

2.  **Подготовка данных:**
    *   Адаптируйте класс `SignalDataset` в `src/dataloader/dataloader.py` для загрузки ваших данных.
    *   Ваш датасет должен возвращать словарь, содержащий как минимум: `signals`, `element_ids`, `session_ids`, `keypoints`.

3.  **Конфигурация:**
    *   Откройте основной скрипт (`train.py`).
    *   Настройте гиперпараметры в блоке `if __name__ == '__main__':`. Ключевые параметры для настройки:
        *   `P_ELEMENTS`, `K_SAMPLES`: Параметры P-K для семплера.
        *   `EMBEDDING_DIM`: Размерность итоговых векторов.
        *   `LEARNING_RATE`: Скорость обучения.
        *   `lambda_metric`, `margin_session`, `margin_element`: Параметры иерархической функции потерь.

4.  **Запуск обучения:**
    ```bash
    python train.py
    ```

5.  **Использование (Inference):**
    *   Загрузите веса обученной модели.
    *   Переведите модель в режим оценки: `model.eval()`.
    *   Подайте на вход тензор с одним или несколькими сигналами.
    *   Выход `embeddings` будет содержать векторные представления.
    *   Выход `keypoint_logits` будет содержать "карты вероятностей". Для нахождения позиций примените `torch.argmax(keypoint_logits, dim=1)`.